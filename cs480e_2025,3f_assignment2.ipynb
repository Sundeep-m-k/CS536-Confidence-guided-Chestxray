{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sundeep-m-k/CS536-Confidence-guided-Chestxray/blob/main/cs480e_2025%2C3f_assignment2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2\n",
        "**Due November 16th, 11:59 PM**\n",
        "\n",
        "GitHub Classroom assignment link:\n",
        "https://classroom.github.com/a/DlsFesvO.\n",
        "\n",
        "Instructions for how to connect your Google Colab to GitHub are [here](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)."
      ],
      "metadata": {
        "id": "tUheapJCOaxW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Name: Sundeep Muthukrishnan Kumaraswamy<br>\n",
        "B-Number: B01105889<br>\n",
        "Email: smuthukrishn@binghamton.edu"
      ],
      "metadata": {
        "id": "26iemMN3qq0v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following assignment, you will be using the deep learning framework, PyTorch, to perform computer vision tasks such as image classification and object detection. This will be a report-style assignment, where you will try multiple different models, optimization algorithms, and hyperparameters, and present your findings in a short report with visualizations inside the notebook.  \n",
        "\n",
        "\n",
        "Functions and cells that need to be implemented are marked with a bold **implement** keyword or clearly marked in the experiments section.\n",
        "\n",
        "The experiments section for each classifier also need to be implemented. You should follow the instructions above the cell. You may also add additional cells.\n",
        "\n",
        "Cells marked **run** need to be run to set up the appropriate infrastructure, but do not need to be modified. Make sure you have run the previous cells before running the current cell, or you may get an error.\n",
        "\n",
        "It is standard practice in ML to share notebooks to discuss the workflow and results in a professional setting. So, the code quality also matters. You should make sure your code is readable and conforms to standard practices. Your figures should be intelligable and include proper axis labels, titles, and legends. Unreadable and poorly written code may result in a points deduction.  \n",
        "\n",
        "Submission will be via GitHub Classroom. **You are required to have at least 15 commits for this assignment.**"
      ],
      "metadata": {
        "id": "FANk9dekbYM0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading and Downloading Data from Colab\n",
        "\n",
        "Unlike the previous assignments, you will downloading and uploading additional data from and into the Colab environment.\n",
        "\n",
        "You can mount directories from your Google Drive and use the session storage for your work.\n",
        "\n",
        "[Take a look here for an example notebook on handling data download and upload on Colab. ](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=hauvGV4hV-Mh)"
      ],
      "metadata": {
        "id": "WAzKOfYiS5JK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import statements\n",
        "\n",
        "**Run** the cell to import the packages needed for the code below. You may other packages but ask first."
      ],
      "metadata": {
        "id": "u6edQRQycJJv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cny_JSAkGykH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)  # Set the seed for the random number generator"
      ],
      "metadata": {
        "id": "SBvLHg2PeHa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce81591b-f818-493d-c93d-774cd44ad00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb7fda93210>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CUDA Runtime\n",
        "\n",
        "You will want to make use of the GPU runtimes on Colab to speed up your training. You can change your runtime by going to:\n",
        "\n",
        "`Runtime > Change runtime type` and selecting GPU.\n",
        "\n",
        "You will have to explicitly use the send Torch tensors to GPUs, by calling `.cuda()`  on the tensors and modules to utilize them on the GPU.\n",
        "\n",
        "[Take a look at the quickstart for PyTorch](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)"
      ],
      "metadata": {
        "id": "tVBtO8uWVqua"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHalgZKOVrMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1: Data Handling in PyTorch (20 pts)"
      ],
      "metadata": {
        "id": "31UCwNBSercl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset class\n",
        "\n",
        "A large part of any machine learning workflow is the proper and efficient handling of data. Datasets are often large, scattered across filesystems, and require transformations and augmentation. Deep learning libraries such as `PyTorch` provide utilities to help in this process. In the next section, you will write a custom MNIST dataset and add data augmentation to your data pipeline for your traininig."
      ],
      "metadata": {
        "id": "i_2o6fS-e8ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Run** the following cell to define some helper functions to load the MNIST data."
      ],
      "metadata": {
        "id": "M0gworgQibYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _data_parser_helper(csv_file_name):\n",
        "  '''\n",
        "  Reads CSV file and converts it into numpy arrays.\n",
        "\n",
        "  Args:\n",
        "    csv_file_name (string): String of the path of csv file.\n",
        "\n",
        "  Returns:\n",
        "    (np.array(float), np.array(int)): Returns a tuple of numpy arrays.\n",
        "  '''\n",
        "  X = []\n",
        "  Y = []\n",
        "  with open(csv_file_name,'r') as _file:\n",
        "      csv_reader = csv.reader(_file, delimiter=\",\")\n",
        "      for row in csv_reader:\n",
        "          Y.append(int(row[0]))\n",
        "          X.append([float(i)/255.0 for i in row[1:]])\n",
        "  return (np.array(X), np.array(Y))\n",
        "\n",
        "def get_mnist_train_data():\n",
        "  X_train, Y_train = _data_parser_helper(\"sample_data/mnist_train_small.csv\")\n",
        "  return X_train, Y_train\n",
        "\n",
        "def get_mnist_test_data():\n",
        "  X_test, Y_test = _data_parser_helper(\"sample_data/mnist_test.csv\")\n",
        "  return X_test, Y_test"
      ],
      "metadata": {
        "id": "12py4n6GiYOB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the `MNIST` class to serve as a container for our PyTorch MNIST data. [Take a look at this tutorial on PyTorch datasets, dataloading, and transforms.](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class)\n",
        "[This second, more important tutorial specifically covers custom datasets and dataloaders.](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html#creating-a-custom-dataset-for-your-files)\n",
        "\n",
        "The MNIST data on Colab is pre-installed on all notebooks as a CSV.\n",
        "Your class must read the data and store it as PyTorch float tensors.\n",
        "Use the helper functions above (already written) to read the data,\n",
        "which will return the data as NumPy arrays.\n",
        "\n",
        "You should implement:\n",
        "\n",
        "- `__init__()` to read the appropriate CSV file, and store it in the class as a `torch.Tensor` with float dtype.\n",
        "- `__len__()` to return the number of samples in the dataset.\n",
        "- `__getitem__(i)` to return the i-th sample and label from the data you have stored."
      ],
      "metadata": {
        "id": "3-Gm0Tyeigj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MNIST(Dataset):\n",
        "  \"\"\"MNIST custom dataset that reads the CSV file and transforms them into PyTorch Tensors\"\"\"\n",
        "\n",
        "\n",
        "  def __init__(self, is_training=True, transform=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      is_training (bool): If true loads\n",
        "        the training dataset. If false, loads the test dataset.\n",
        "        Use the functions above.\n",
        "      transform (callable): Transform to be applied on a sample.\n",
        "        These will be used for data augmentations.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self._data, self._label = get_mnist_train_data()\n",
        "\n",
        "    self._data = torch.from_numpy(self._data.reshape((20000, 1, 28, 28)))\n",
        "    self._label = torch.from_numpy(self._label)\n",
        "    self.transform = transform\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    \"\"\"Returns the size (the number of samples) of the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # raise NotImplementedError(\"The __len__() method was not implemented.\")\n",
        "    return self._data.shape[0]\n",
        "\n",
        "  def __getitem__(self, i):\n",
        "    \"\"\"Returns the i-th sample and label and applies any transforms defined.\n",
        "\n",
        "      Args:\n",
        "        i (int): The index of sample in the data array to retrieve.\n",
        "    \"\"\"\n",
        "\n",
        "    # return NotImplementedError(\"The __getitem__() method was not implemented\")\n",
        "\n",
        "    sample = self._data[i]\n",
        "    label = self._label[i]\n",
        "    if (self.transform):\n",
        "      semple = self.transform(sample)\n",
        "    return sample, label"
      ],
      "metadata": {
        "id": "FF0eE-oge8F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "composed = transforms.Compose([transforms.RandomRotation(15)])\n",
        "mnist_dataset = MNIST(transform=composed)"
      ],
      "metadata": {
        "id": "wPQKDhWwyiz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import perf_counter"
      ],
      "metadata": {
        "id": "yd-nczuCzfTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = DataLoader(mnist_dataset, batch_size=512, num_workers=2)"
      ],
      "metadata": {
        "id": "xeCymG38zH3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = perf_counter()\n",
        "for _data, label in loader:\n",
        "  x = _data.shape[0]\n",
        "end = perf_counter()"
      ],
      "metadata": {
        "id": "p1JutL6nzRv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHUCp0GgzqxX",
        "outputId": "5621b2cc-4b8b-41b6-a034-e9fa396ef12d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5.6844704809999484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the following cell to visualize the data in the dataset. Use `Matplotlib` or your favorite visualization package to plot 5 images of each class in a single figure.\n",
        "\n",
        "You should initialize a MNIST dataset object for the training class. Visualize this dataset. This will not be used in the future.  \n",
        "\n"
      ],
      "metadata": {
        "id": "fy6B1261lufJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fGhUhWsLervV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement***\n",
        "the following cell to create a PyTorch transform object and\n",
        "assign it to a variable.\n",
        "You may name the variable whatever you wish;\n",
        "you will use the object to construct train and test MNIST datasets.\n",
        "\n",
        "You will use the transformations defined in `torchvision`,\n",
        "which can be found [here](https://pytorch.org/vision/stable/transforms.html#transforms-on-pil-image-and-torch-tensor).\n",
        "\n",
        "Add the `RandomRotation` and `Normalize` transformations to the dataset. You will need to [compose the two transforms](https://pytorch.org/vision/0.9/transforms.html#torchvision.transforms.Compose). Restrict the rotations to +/- 15 degrees.\n",
        "\n",
        "The mean and standard deviation of the training set is 0.13 and 0.31 respectively.\n",
        "\n",
        "At the end of the cell, construct a training and test dataset.\n",
        "Name these carefully, these datasets will be used in the next sections."
      ],
      "metadata": {
        "id": "Y14bBS2fmbEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform  = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(degrees = (15)),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "    ])\n",
        "\n",
        "mnist_train = MNIST(is_training=True, transform=transform)\n",
        "mnist_test = MNIST(is_training=False, transform=transform)"
      ],
      "metadata": {
        "id": "jtKGgyI3mbPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the following cell to visualize the data in the dataset with the transformations similar to the previous visualization."
      ],
      "metadata": {
        "id": "ANOuWwmSnPkF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G9D3b2wpnUTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the following cell to randomly split the training dataset previously defined, and make a training and validation set. Do an 80-20 split for the training and validation set. Name these sets carefully, they will be used in the next section to train your models.\n",
        "\n",
        "You can use the utilities in `torch.utils.data`."
      ],
      "metadata": {
        "id": "FqND0a-DngYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "train_len = int(0.8 * len(mnist_train))\n",
        "val_len = len(mnist_train) - train_len\n",
        "train_set, val_set = random_split(mnist_train, [train_len, val_len])"
      ],
      "metadata": {
        "id": "QIT0omqUngq8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2: MNIST Classification (40 pts)"
      ],
      "metadata": {
        "id": "naiZzb4OcipT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Classifiers\n",
        "\n",
        "The linear classifiers you implemented in the last assignment will serve as our baseline for more powerful convolutional neural networks. You will ***implement*** the multi-class SVM and  Softmax classifiers in the following cells.  "
      ],
      "metadata": {
        "id": "VHaGaWsQcuWa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** a LinearSVM classifier in the following cell. Your implementation should be a python class that inherits from `torch.nn.Module`.\n",
        "\n",
        "You should use classes and functions defined in `torch.nn` and `torch.nn.Functional`.   "
      ],
      "metadata": {
        "id": "Rrg6BMCWdaTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearSVM(nn.Module):\n",
        "  ''' Implements the linear SVM using Torch.\n",
        "  '''\n",
        "  def __init__(self, num_classes, *args, **kwargs):\n",
        "    '''\n",
        "      num_classes (int): The number of output classes\n",
        "    '''\n",
        "    raise NotImplementedError(\"LinearSVM forward not implemented\")\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "      x (torch.Tensor): Input image as a torch tensor.\n",
        "    '''\n",
        "    raise NotImplementedError(\"LinearSVM forward not implemented\")"
      ],
      "metadata": {
        "id": "VdQU1itSctyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** using any PyTorch defined optimizer, code to train your LinearSVM classifier on the training dataset. You will want to try different optimizers, learning rates, and batch sizes.\n",
        "\n",
        "PyTorch provides implementations of various optimization algorithms in the `torch.optim` package. You can use any of the first order methods such as:\n",
        "\n",
        "- SGD (with or without momentum)\n",
        "- AdaDelta\n",
        "- ADAM\n",
        "- RProp\n",
        "- RMSProp\n",
        "\n",
        "***Note:*** You should keep track of the performance of the optimizers, batch sizes, and learning rates. You should justify the choice of your hyperparameters in the report at the end of this section. You will be asked to quantify and visualize the differences between these hyperparameters.\n",
        "\n",
        "You should use `Torch.utils.data.DataLoader` to do the data loading. Make sure to use the correct loss function. You may use the predefined loss functions available in `Torch.nn`.\n",
        "\n",
        "***Note:*** As the optimization procedure for training different models is often the same, you should write helper functions that are reusable. This will make your code more readable and reduce the possibility of unexpected bugs.  \n",
        "\n",
        "To train a single model, you will need to:\n",
        "\n",
        "- Iterate through the data in batches using your training or validation dataloader\n",
        "- Perform the forward pass\n",
        "- Compute the loss\n",
        "- Perform the backward pass\n",
        "- Take an optimizer step\n",
        "- Repeat the process till network converges or some other criterion\n",
        "- Every n iterations, go through the validation set and calculate the loss and accuracy of the validation set to check your training performance"
      ],
      "metadata": {
        "id": "9zwabAUlda_i"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tg9aAl_JchSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** a Softmax classifier in the following cell. Your implementation should be a python class that inherits from `torch.nn.Module`."
      ],
      "metadata": {
        "id": "VeL39QqOnako"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u5p2C_1QHSb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** using any PyTorch defined optimizer, train your Softmax classifier on the training dataset and validate on the validation dataset, as you did before.\n",
        "\n",
        "***Note:*** You should keep track of the performance of the optimizers, batch sizes, and learning rates. You should justify the choice of your hyperparameters in the report at the end of this section. You will be asked to quantify and visualize the differences between these hyperparameters.\n",
        "\n",
        "You will want to try different optimizers, learning rates, and batch sizes. You should use `Torch.DataLoader` to simplify the data loading. Make sure to use the correct loss function. You may use predefined loss functions available in `Torch.nn`.\n",
        "\n",
        "Same as before, to train a single model, you will need to:\n",
        "\n",
        "- Iterate through the data in batches using your training or validation dataloader\n",
        "- Perform the forward pass\n",
        "- Compute the loss\n",
        "- Perform the backward pass\n",
        "- Take an optimizer step\n",
        "- Repeat the process till network converges or some other criterion\n",
        "- Every n iterations, go through the validation set and calculate the loss and accuracy of the validation set to check your training performance"
      ],
      "metadata": {
        "id": "LgLiSrkvov7_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LFAf4i-cowF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** visualizations in the following cell to show performance differences of the different hyperparameters.\n",
        "\n",
        "***Include*** visualizations for the effect of different batch sizes, learning rates, and optimizers. You are free to choose any method of visualization as long as it is able to succintly convey your justifications."
      ],
      "metadata": {
        "id": "AjBRu6jJp4Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XbKSedbmp3lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Neural Networks"
      ],
      "metadata": {
        "id": "WWSOkLP69Tv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the following cell to implement a convolutional neural network. Similar to before, your implementation should be a Python class that inherits from `torch.nn.Module`.\n",
        "\n",
        "Your model can use any of the building blocks defined in `torch.nn` including but not limited to:\n",
        "\n",
        "- Conv2D\n",
        "- Linear\n",
        "- Activation layers:\n",
        "  - ReLU\n",
        "  - Tanh\n",
        "  - Sigmoid\n",
        "  - Softmax\n",
        "- Normalization layers:\n",
        "  - BatchNorm\n",
        "  - LayerNorm\n",
        "  - GroupNorm\n",
        "- Pooling layers:\n",
        "  - MaxPool\n",
        "  - AvgPool\n",
        "- Dropout\n",
        "\n",
        "Make sure to chose the correct dimensional versions of the layers, i.e for images use MaxPool2d and vectors use MaxPool1d\n"
      ],
      "metadata": {
        "id": "tFAJcXU-owUO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example LeNet implementation:\n",
        "\n",
        "```python\n",
        "class LeNet(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(LeNet, self).__init__()\n",
        "        \n",
        "        # num_classes number of output classes\n",
        "        self.num_classes = num_classes\n",
        "        # 1 input image channel,\n",
        "        # 6 output channels\n",
        "        # 5x5 square convolution kernel\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # ReLU activation function\n",
        "        self.act1 = nn.ReLU()\n",
        "        # Max pooling with 2x2 square kernel\n",
        "        self.pool1 = nn.MaxPool2d(2)\n",
        "        \n",
        "        # Second convolution with 6 input image channels\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.act2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2)\n",
        "\n",
        "        # 3 Layer fully connected layer\n",
        "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "        self.act3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.act4 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        # First Convolution -> Activation -> Max Pooling\n",
        "        x = self.conv1(x)\n",
        "        x = self.act1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # Second Convolution -> Activation -> Max Pooling\n",
        "        x = self.conv2(x)\n",
        "        x = self.act2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # flatten all dimensions except the batch dimension\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        # Three layer MLP with ReLU activations\n",
        "        x = self.act3(self.fc1(x))\n",
        "        x = self.act4(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "AU2zciw2y9tZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cells, perform experiments using the CNN you defined. You may need to reshape your data in order to 2D convolutions.\n",
        "\n",
        "Your objective is to find the best architecture that minimizes the error but using as few a parameters and FLOPS as possible.\n",
        "\n",
        "***Performance matters!*** The model with the best combination of high accuracy, parameter, and compute efficiency will get extra credit (15 pts)."
      ],
      "metadata": {
        "id": "8TT1pSu2ppPd"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5bzZVDfwppcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the following cell to compute and compare the test set performance (accuracy) of the linear classifiers and CNN."
      ],
      "metadata": {
        "id": "tQGx_PUwdLlX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kzt_M3dqdLzG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cells ***write*** a short justification on how you came up with your final model and architecture. You may include details like:\n",
        "\n",
        "- Which optimizer did you use? Why did you use that?\n",
        "- What about the other hyperparameters?\n",
        "- Did you base your model on an existing architecture?\n",
        "- What changes did you make to improve accuracy?\n",
        "- What changes did you make to improvde performance?\n",
        "\n",
        "***Include*** visualizations for the effect of different batch sizes, learning rates, and optimizers. You are free to choose any method of visualization as long as it is able to succintly convey your justifications."
      ],
      "metadata": {
        "id": "uhb5-NPKr6RQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tpaxF_nWr6go"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3: Real World Data (40 pts)"
      ],
      "metadata": {
        "id": "KzbKJhaarIae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While test set accuracy is meant to approximate the perfomance in real world data, biases in data collection and processing often result in inaccurate performance estimates. In this section, you will be using the previously defined models to re-train on the data the class has collected.\n",
        "\n",
        "In user systems, noisy inputs are often likely. To make your model robust to faulty input, you will need to add an additional class to your model which corresponds to the label 10. This label is reserved for inputs that are **not** digits.\n"
      ],
      "metadata": {
        "id": "xlqE7pdPtiMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the following cells to load and convert the new dataset and combine the new dataset with old dataset.  \n",
        "\n",
        "Make sure you have downloaded the `MNIST580E_training.csv` and `MNIST580E_test.csv` files from the shared drive and uplodaded it to your GDrive. You will be combining our collected data with the MNIST dataset available with Colab.\n",
        "\n",
        "You will need to:\n",
        "\n",
        "\n",
        "- Mount your GDrive\n",
        "- Load the `MNIST580E_training.csv` and `MNIST580E_test.csv` files\n",
        "- Parse the files and create train and test datasets as you did previously\n",
        "  - Make sure to normalize the data and add data augmentation. You are free to use any augmentation you'd like.\n",
        "- Combine the new datasets with the dataset available on Colab.\n",
        "  - You can use the `ConcatDataset` or `ChainDataset`.  \n",
        "- Visualize the newly created data. This should serve as a sanity check for your newly written data pipeline.\n",
        "\n",
        "**Data Augmentation:**\n",
        "\n",
        "The collected data will have more variability compared to the original dataset.\n",
        "So you should make heavy use of data augmentation on the collected dataset. You can use augmentations such as as:\n",
        "\n",
        "- Rotation\n",
        "- Resize\n",
        "- Blur\n",
        "- Perspective shift\n",
        "\n",
        "**Another hint:** Since we are adding an additional class type (not a digit), our dataset will be imbalanced. There will be fewer samples with that label compared to other labels. You may want to \"double count\" (i.e duplicate) the not-a-digit samples in your collected dataset."
      ],
      "metadata": {
        "id": "twnfeBTdwjPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9--R1a9dwjZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the following cell to use your previously trained CNN as a feature extractor. This is an example of [transfer learning](https://en.wikipedia.org/wiki/Transfer_learning).\n",
        "\n",
        "You will need to:\n",
        "- Freeze the layers of convolutional model you trained\n",
        "- Change the final linear layer have an additional output for the additional class.  \n",
        "\n",
        "Here's an example of taking the previously defined LeNet and updating the final layer to have 11 classes instead of 10. This also freezes the other weights in the LeNet.\n",
        "\n",
        "```python\n",
        "# model_conv is a trained LeNet example from above\n",
        "\n",
        "# Freeze the weights of the model\n",
        "# The gradients will not be calculated and the optimizer will not\n",
        "# update the weights\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of newly constructed modules have requires_grad=True by default\n",
        "num_ftrs = model_conv.fc3.in_features\n",
        "model_conv.fc3 = nn.Linear(num_ftrs, 11)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Only parameters of final layer are being optimized not the other weights\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001)\n",
        "```\n"
      ],
      "metadata": {
        "id": "mzY-itbyZvtD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nBZbXC8SZv8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** the following cell to train the final layer of the convolution. Also calculate the test set performance on this fine-tuned model."
      ],
      "metadata": {
        "id": "6Sp7sQjub7HY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Izl74Ydtb8-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the following cell to retrain the linear classifiers (LinearSVM and Softmax). Note that you have an extra class as your model should also detect \"not a digit\"."
      ],
      "metadata": {
        "id": "cQyYbK6AuqUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UKvrkj9uupeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Implement*** the following cell to retrain your previously defined CNN model."
      ],
      "metadata": {
        "id": "dEKnIfUnu0DF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ovyuauM65eA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implement** a visualization of the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) on the **new** combined test set with the predictions from your LinearSVM, Softmax, and CNN. You should use a single `Matplotlib` figure with multiple subplots on a single row.\n",
        "\n",
        "[See here for examples on using plt.subplots.](https://matplotlib.org/stable/gallery/subplots_axes_and_figures/subplots_demo.html)"
      ],
      "metadata": {
        "id": "dhtx446K5FNI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q_rCWjbVupm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following cells, write a short report about the performance of your models. You should include the effects of new data, data augmentation, and different architectures. You should also include visualizations as you did before."
      ],
      "metadata": {
        "id": "xxv4Zf51wSpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HnMP5lDUwTd4"
      }
    }
  ]
}